#To display in numbers and not as E+4
pd.options.display.float_format = '{:,.2f}'.format

https://www.kaggle.com/marcogdepinto/titanic-with-python-first-competition 
train = pd.read_csv('titanic_train.csv') 
test = pd.read_csv('titanic_test.csv') 
combined = [train, test] 
for dataset in combined: 
    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int) 

train['Cabin'].fillna('U', inplace=True) 
train['Cabin1'] = train['Cabin'].apply(lambda x: x[0]) 
train['Cabin1'].unique() 
replacement = { 
    'T': 0, 
    'U': 1, 
    'A': 2, 
    'G': 3, 
    'C': 4, 
    'F': 5, 
    'B': 6, 
    'E': 7, 
    'D': 8 
} 

train['Cabin2'] = train['Cabin1'].apply(lambda x: replacement.get(x)) 

#To find 
train_df[["Sex", "Survived"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)
	Sex	Survived
0	female	0.742038
1	male	0.188908

#Extracting text from column
for dataset in combine:
    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)
-----
dataset['Title'] = dataset['Name'].str.split(", ", expand=True)[1].str.split(".", expand=True)[0]

Identifying band based	
train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)
train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)

FareBand	Survived
0	(-0.001, 7.91]	0.197309
1	(7.91, 14.454]	0.303571
2	(14.454, 31.0]	0.454955
3	(31.0, 512.329]	0.581081

for dataset in combine:
    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0
    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1
    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2
    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3
    dataset['Fare'] = dataset['Fare'].astype(int)

train_df = train_df.drop(['FareBand'], axis=1)
combine = [train_df, test_df]
    
train_df.head(10)

#Filling nulls:
# only for test_df, since there is a missing "Fare" values
test_df["Fare"].fillna(test_df["Fare"].median(), inplace=True)

#####################https://www.kaggle.com/omarelgabry/a-journey-through-titanic?scriptVersionId=447794
#Handling null using mean and std
# Age 
fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))
axis1.set_title('Original Age values - Titanic')
axis2.set_title('New Age values - Titanic')

# axis3.set_title('Original Age values - Test')
# axis4.set_title('New Age values - Test')

# get average, std, and number of NaN values in titanic_df
average_age_titanic   = titanic_df["Age"].mean()
std_age_titanic       = titanic_df["Age"].std()
count_nan_age_titanic = titanic_df["Age"].isnull().sum()

# get average, std, and number of NaN values in test_df
average_age_test   = test_df["Age"].mean()
std_age_test       = test_df["Age"].std()
count_nan_age_test = test_df["Age"].isnull().sum()

# generate random numbers between (mean - std) & (mean + std)
rand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, size = count_nan_age_titanic)
rand_2 = np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)

# plot original Age values
# NOTE: drop all null values, and convert to int
titanic_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)
# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)

# fill NaN values in Age column with random values generated
titanic_df["Age"][np.isnan(titanic_df["Age"])] = rand_1
test_df["Age"][np.isnan(test_df["Age"])] = rand_2

# convert from float to int
titanic_df['Age'] = titanic_df['Age'].astype(int)
test_df['Age']    = test_df['Age'].astype(int)
        
# plot new Age Values
titanic_df['Age'].hist(bins=70, ax=axis2)
# test_df['Age'].hist(bins=70, ax=axis4)

freq_port = train_df.Embarked.dropna().mode()[0] #finds most occurance 
freq_port
for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)

train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)
for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)

#To handle null using multiple values
guess_ages = np.zeros((2,3))
guess_ages

for dataset in combine:
    for i in range(0, 2):
        for j in range(0, 3):
            guess_df = dataset[(dataset['Sex'] == i) & \
                                  (dataset['Pclass'] == j+1)]['Age'].dropna()

            # age_mean = guess_df.mean()
            # age_std = guess_df.std()
            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)

            age_guess = guess_df.median()

            # Convert random age float to nearest .5 age
            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5
            
    for i in range(0, 2):
        for j in range(0, 3):
            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\
                    'Age'] = guess_ages[i,j]

    dataset['Age'] = dataset['Age'].astype(int)

train_df.head()

train_df['AgeBand'] = pd.cut(train_df['Age'], 5)
train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)

for dataset in combine:    
    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0
    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1
    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2
    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3
    dataset.loc[ dataset['Age'] > 64, 'Age']
train_df.head()    

######################
def get_person(passenger):
    age,sex = passenger
    return 'child' if age < 16 else sex
    
titanic_df['Person'] = titanic_df[['Age','Sex']].apply(get_person,axis=1)

###################################################
#Data cleaning
# Age
df.loc[(df.Age.isnull())&(df.Title=='Mr'),'Age']= df.Age[df.Title=="Mr"].mean()
df.loc[(df.Age.isnull())&(df.Title=='Mrs'),'Age']= df.Age[df.Title=="Mrs"].mean()
df.loc[(df.Age.isnull())&(df.Title=='Master'),'Age']= df.Age[df.Title=="Master"].mean()
df.loc[(df.Age.isnull())&(df.Title=='Miss'),'Age']= df.Age[df.Title=="Miss"].mean()
df.loc[(df.Age.isnull())&(df.Title=='Other'),'Age']= df.Age[df.Title=="Other"].mean()

#Data cleaning for categorical 
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode().iloc[0])

#Feature Engineering
## Assign Binary to Sex str
df['Sex'] = df['Sex'].map({'female': 1,
                           'male': 0}).astype(int)

#Functions
def trip_type(data,cnt):
    if(data.split(" ")[0]==data.split(" ")[cnt]):
        return 'R'
    else:
        return 'O'

data['trip_type']=data.apply(lambda row: trip_type(row['ROUTE(SECTORS)'], row['ROUTE(SECTORS)_CNT']), axis=1)
-------------
import re
at = re.compile(r" ", re.I)
def count_hypen(string):
    count = 0
    for i in at.finditer(string):
        count += 1
    return count
data['ROUTE(SECTORS)_CNT']=data["ROUTE(SECTORS)"].map(count_hypen)		
gwp['PROPOSED_FARE_USD']  = gwp.apply(lambda x: toUSD(x['PROPOSED_FARE_USD']), axis=1)
------------------------------
pnr['LeadDays0_2'] = np.where((pnr['LeadDays'] >=0) & (pnr['LeadDays'] <=2), 1, 0)
region_type=pnr.groupby('REGION_NAME').agg({'REC_LOCATOR':'count'})

#Numbering row number from 1 to n rows
pnr['val_id'] = range(1, len(pnr) + 1)

#To concat 2 columns
gwp['PROPOSED_FARE_USD'] = gwp[['CURRENCY', 'PROPOSED_FARE']].apply(lambda x: ''.join(x), axis=1)
-----------------
data['TOUR_CODE_IND'] = data.apply(lambda x: 1 if x['TOUR CODE']== x['ORIGINAL TOUR CODE'] else 0, axis=1)
-----------------
def AGENT_CHECK(string):
    if(string.find("EMIRATES")<>-1):
        return 'EMIRATES GROUP'
    elif(string.find("DNATA")<>-1):
        return 'EMIRATES GROUP'
    elif(string.find("AIRPORT")<>-1):
        return 'AIRPORT BOOKING'
    else:
        return 'REGULAR'
    
data['S/L DESC Grped']=data["S/L DESC"].map(AGENT_CHECK)
---------------------
GCC_COUNTRIES=['UNITED ARAB EMIRATES','SAUDI ARABIA','KUWAIT','OMAN','QATAR']
def GCC_COUNTRY_CHECK(string):
    if any(string in s for s in GCC_COUNTRIES):
        return 'GCC'
    else:
        return 'REGULAR'    
data['GCC_COUNTRY_Grped']=data["COUNTRY NAME"].map(GCC_COUNTRY_CHECK)
------------------
dataTop25Items_unstack = dataTop25Items_unstack.applymap(lambda x: 0 if x<=0 else 1)
---------------------
data['TICKETED FARE BASIS TYPE1']=data['TICKETED FARE BASIS'].str.split("/",expand=True)[0]
data['TICKETED FARE BASIS TYPE2']=data['TICKETED FARE BASIS'].str.split("/",expand=True)[1]
def TICKET_FARE_CHECK(string):
    if not string:
        return 0
    else:
        return 1
    
data['TICKETED FARE_IND']=data["TICKETED FARE BASIS TYPE2"].map(TICKET_FARE_CHECK)
-----------
#To find null
print(df.isnull().sum())
-------
print(y.value_counts(normalize=True)*100)
---------------------------
#comma column to rows
data2=data1.set_index('REC_LOCATOR').CHILD_PNRS.str.split(',', expand=True).stack().reset_index('REC_LOCATOR')

data['TOUR_CODE_IND'] = data.apply(lambda x: 1 if x['TOUR CODE']== x['ORIGINAL TOUR CODE'] else 0, axis=1)

#rows to columns
dataTop25Items_pivot = dataTop25Items[['Date','Item','Transaction']].pivot_table('Transaction', 'Date', 'Item')
hot_encoded_df=data.groupby(['Transaction','Item'])['Item'].count().unstack().reset_index().fillna(0).set_index('Transaction')

----------------------------------------
data=pd.read_excel('GROUP_BKNG_PNR.xlsx')
data1=data[['REC_LOCATOR','CHILD_PNRS']]
data1.set_index('REC_LOCATOR',inplace=True)
data1=data1.CHILD_PNRS.str.split(',',expand=True).stack().reset_index('REC_LOCATOR')
data1.to_csv('Parent_Child.csv',index=False)
-----------
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']

newdf = df.select_dtypes(include=numerics)
----------
sepcols = [x for x in highvalue_customers.columns if 'sep_' in x]
----------
corrmat = np.corrcoef(moucols_6_data.transpose())
corrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())
print("max corr:",corrmat_nodiag.max(), ", min corr: ", corrmat_nodiag.min(),)
-------------
def correlation(dataset, threshold):
    col_corr = set() # Set of all the names of deleted columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if corr_matrix.iloc[i, j] >= threshold:
                colname = corr_matrix.columns[i] # getting the name of column
                col_corr.add(colname)
                if colname in dataset.columns:
                    del dataset[colname] # deleting the column from the dataset
    print(col_corr)
    print(dataset)
correlation(moucols_6_data,.80)
----------
import pandas as pd
pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

pd.to_datetime(data['submit_time'],format='%m/%d/%y-%H:%M:%S')
data['PNR_CREATION_YEAR']=data.PNR_CREATION_DATE.dt.year
data['PNR_CREATION_MONTH']=data.PNR_CREATION_DATE.dt.month
data['PNR_CREATION_Y0M'] = data[['PNR_CREATION_YEAR','PNR_CREATION_MONTH']].dot([100,1])
df['C'] = (df['B'] - df['A']).dt.days
data['issue_yearmonth']=(data['issue_d'].dt.year*100)+data['issue_d'].dt.month